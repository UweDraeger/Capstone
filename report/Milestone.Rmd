---
title: "Milestone Report"
author: "Uwe Draeger"
date: "08/05/2021"
output: html_document
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(tidytext)

knitr::opts_chunk$set(echo = TRUE)

```

## Introduction

This milestone report will provide some information on the project thus far. It
will use the tidytext package from authors David Robinson and Julia Silge as
much as possible.

## Data

There are 3 data sets provided - from twitter, from blogs, and from news feeds.
The files are unstructured in the sense that each line holds a single chunk of
text of variable length.

### Loading

Data is read with readr::read_lines which preserves the structure of the data
with one line. The function allows to limit the number of lines read via the
n_max parameter. This may come handy when developing the application as the full
data sets are getting rather large. However, for the milestone report the full
sets will be used.

Specific path and file names are set to load all 3 data files for further
analysis.

```{r load, include=FALSE}

# assign paths and file names
setwd("~/Coursera/Scripts and Data/Capstone")
data_dir <- file.path(getwd(), "datasets", "final", "en_US")

tw_file_name <- "en_US.twitter.txt"
bl_file_name <- "en_US.blogs.txt"
nw_file_name <- "en_US.news.txt"

# read the data 
twitter_data <- read_lines(paste0(data_dir, "/", tw_file_name),
                           skip_empty_rows = TRUE)
blogs_data <- read_lines(paste0(data_dir, "/", bl_file_name),
                   skip_empty_rows = TRUE)
news_data <- read_lines(paste0(data_dir, "/", nw_file_name),
                   skip_empty_rows = TRUE)

```

```{r table1}

# prepare a tibble
mytable1 <- tribble(
        ~data_source, ~number_lines,
        tw_file_name, length(twitter_data),
        bl_file_name, length(blogs_data),
        nw_file_name, length(news_data)
        )
mytable1

```

### Cleaning

The first step is to "tokenize" each data set (corpora) into smaller pieces.

We start at the word level which will allow us to analyze the count and
frequency of the words in each corpus.

Using the tokenization process provided with the tidytext package immediately
strips punctuation while converting all letters to lower case. In order to
reduce the number of words to "real" words we remove all words that do not start
with a letter.

```{r tokenization}

# prepare tibbles 
tw_text <- tibble(line = 1:length(twitter_data), text = twitter_data)
bl_text <- tibble(line = 1:length(blogs_data), text = blogs_data)
nw_text <- tibble(line = 1:length(news_data), text = news_data)

# tokenize these tibbles - separate into words 
tw_words <- tw_text %>%
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE)
bl_words <- bl_text %>% 
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE)
nw_words <- nw_text %>% 
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE)

```

For some kinds of analyses the removal of so-called stop words is recommended.
Stop words are the most common words like "the", "of", "to", etc. which may not
add value in certain analyses.\
In our case, removing these stop words may be counter-productive given the
attempt to build a tool to predict the next word.

## Exploratory Analysis

Table 2 shows the total number of lines and words extracted.

```{r table2}

# prepare tibble
mytable2 <- tribble(
        ~data_source, ~number_lines, ~total_number_words,
        "twitter", length(twitter_data), nrow(tw_words), 
        "blogs", length(blogs_data), nrow(bl_words), 
        "news", length(news_data), nrow(nw_words)
        )
mytable2

```

Given the much larger number of lines in the twitter data there are comparably
few words. This is likely caused by twitter's limited number of characters per
post.

In a next step we count the number of unique words and sort the data by frequency.

```{r further_tokenization}

# add unique word count 
tw_words <- tw_words %>%
        count(word, sort = TRUE)
bl_words <- bl_words %>% 
        count(word, sort = TRUE)
nw_words <- nw_words %>% 
        count(word, sort = TRUE)

```

```{r table3}

# prepare tibble
mytable3 <- tribble(
        ~data_source, ~number_unique_words,
        "twitter", nrow(tw_words), 
        "blogs", nrow(bl_words), 
        "news", nrow(nw_words)
        )
mytable3

```

The next tables show the top 10 words.

Twitter

```{r tw_table}

# add frequencies and cumulated frequencies
total_tw_words <- sum(tw_words$n)

tw_words <- tw_words %>%
        mutate(freq = n / total_tw_words) %>%
        mutate(cumfreq = cumsum(freq))

tw_words %>% slice_head(n = 10)
tw_words

```

```{r tw_chart}

tw_words %>% 
  filter(freq > 0.01) %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(x = freq, y = word)) +
  geom_col() +
  labs(title = "Twitter: Words with a frequency greater than 1%",
       y = NULL)

```

blogs
```{r bl_table4}

total_bl_words <- sum(bl_words$n)

bl_words <- bl_words %>% 
        mutate(freq = n / total_bl_words) %>% 
        mutate(cumfreq = cumsum(freq))

bl_words %>% slice_head(n = 10)

```


```{r bl_chart}

bl_words %>% 
  filter(freq > 0.01) %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(x = freq, y = word)) +
  geom_col() +
  labs(title = "Blogs: Words with a frequency greater than 1%",
       y = NULL)

```
news
```{r nw_table4}

total_nw_words <- sum(nw_words$n)

nw_words <- nw_words %>% 
        mutate(freq = n / total_nw_words) %>% 
        mutate(cumfreq = cumsum(freq))

nw_words %>% slice_head(n = 10)

```


```{r nw_chart}

nw_words %>% 
  filter(freq > 0.01) %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(x = freq, y = word)) +
  geom_col() +
  labs(title = "News: Words with a frequency greater than 1%",
       y = NULL)

```


top 10 bigrams plot

```{r tw_2gram_chart}

# create bigrams
tw_2grams <- tw_text %>%
        unnest_tokens(bigram, text, token = "ngrams", n = 2)  %>%
        count(bigram, sort = TRUE)

tw_2grams %>% 
  filter(n > 25000) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = n, y = bigram)) +
  geom_col() +
  labs(title = "Twitter: Bigrams with a count greater than 40k",
       y = NULL)

```
top 10 trigrams plot

```{r tw_3gram_chart}

# create trigrams 
tw_3grams <- tw_text %>%
        unnest_tokens(trigram, text, token = "ngrams", n = 3)  %>%
        count(trigram, sort = TRUE)

tw_3grams %>% 
  filter(n > 5000) %>%
  mutate(trigram = reorder(trigram, n)) %>%
  ggplot(aes(x = n, y = trigram)) +
  geom_col() +
  labs(title = "Twitter: trigrams with a count greater than 5000",
       y = NULL)

```

## Interesting findings

Run time for building twitter 2gram: 3:30 minutes
Run time for building twitter 3gram: 10:07 minutes

different distributions of words and ngrams in the sources 
Q: Any chance to consider the type of text when guessing the next word? maybe dependent on the application (chat more like twitter, wordpress more like blogs, etc.)?



## Future Steps


Use the sbo package to create the application

Shiny: keep simple - 
input: text
count number of words in input text
if greater than 3 use only last 3

output: text
if 
  3gram found show most likely next word (choice of 3)
else
  use last 2 words
  if 2gram found show most likely next words (choice of 3)
  else
    use last 1 word
    if word found show most likely next words (choice of 3)
    else
      show most frequent word (choice of 3)
      
submit button
