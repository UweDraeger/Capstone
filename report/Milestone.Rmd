---
title: "Milestone Report"
author: "Uwe Draeger"
date: "08/05/2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(tidytext)

knitr::opts_chunk$set(echo = TRUE)

```

## Introduction

This milestone report will provide some information on the project 
thus far. It will use the tidytext package from authors David Robinson and Julia Silge as much as possible.


## Data

There are 3 data sets provided - from twitter, from blogs, and from  news feeds. 
The files are unstructured in the sense that each line holds a single chunk of text of variable length.

### Loading

Data is read with readr::read_lines which preserves the structure of the data with one line. The function allows to limit the number of lines read via the n_max parameter. This may come handy when developing the application as the full data sets are getting rather large. However, for the milestone report the full sets will be used.

Specific path and file names are set to load all 3 data files for further analysis. 

```{r load, include=FALSE}

# assign paths and file names
setwd("~/Coursera/Scripts and Data/Capstone")
data_dir <- file.path(getwd(), "datasets", "final", "en_US")

tw_file_name <- "en_US.twitter.txt"
bl_file_name <- "en_US.blogs.txt"
nw_file_name <- "en_US.news.txt"

# read the data 
twitter_data <- read_lines(paste0(data_dir, "/", tw_file_name),
                           skip_empty_rows = TRUE)
blogs_data <- read_lines(paste0(data_dir, "/", bl_file_name),
                   skip_empty_rows = TRUE)
news_data <- read_lines(paste0(data_dir, "/", nw_file_name),
                   skip_empty_rows = TRUE)

```

Data source       | number of lines
------------------|-------------------------
en_US.twitter.txt | `r length(twitter_data)`
en_US.blogs.txt   | `r length(blogs_data)`
en_US.news.txt    | `r length(news_data)`

### Cleaning

The first step is to "tokenize" each data set (corpora) into smaller pieces. 

We start at the word level which will allow us to analyze the count and frequency of the words in each corpus.

Using the tokenization process provided with the tidytext package immediately strips punctuation while converting all letters to lower case. In order to reduce the number of words to "real" words we remove all words that do not start with a letter.

```{r tokenization, include=FALSE}

# prepare tibbles 
tw_text <- tibble(line = 1:length(twitter_data), text = twitter_data)
bl_text <- tibble(line = 1:length(blogs_data), text = blogs_data)
nw_text <- tibble(line = 1:length(news_data), text = news_data)

# tokenize these tibbles - separate into words 
tw_words <- tw_text %>%
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE) %>%
        count(word, sort = TRUE)
bl_words <- bl_text %>% 
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE) %>%
        count(word, sort = TRUE)
nw_words <- nw_text %>% 
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE) %>%
        count(word, sort = TRUE)

```

For some kinds of analyses the removal of so-called stop words is recommended. Stop words are the most common words like "the", "of", "to", etc. which may unnecessarily inflate the analysis base. 
Removing these stop words may not be helpful in our case of attempting to build a prediction tool.


## Exploratory Analysis

line and word counts
average number of words per line
top 10 frequent words in all sources
plot

top 10 bigrams
plot

top 10 trigrams
plot



## Interesting findings

different distributions of words and ngrams in the sources
with / without stopwords
        - what is the percentage of stopwords in the bigrams/trigrams?

## Future Steps

Any chance to consider the type of text when guessing the next word?
maybe dependent on the application (chat more like twitter, wordpress more like blogs, etc. )