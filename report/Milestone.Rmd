---
title: "Milestone Report"
author: "Uwe Draeger"
date: "08/05/2021"
output: html_document
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(tidytext)

knitr::opts_chunk$set(echo = TRUE)

```

## Introduction

This milestone report will provide some information on the project thus far. It
will use the tidytext package from authors David Robinson and Julia Silge as
much as possible.

## Data

There are 3 data sets provided - from twitter, from blogs, and from news feeds.
The files are unstructured in the sense that each line holds a single chunk of
text of variable length.

### Loading

Data is read with readr::read_lines which preserves the structure of the data
with one line. The function allows to limit the number of lines read via the
n_max parameter. This may come handy when developing the application as the full
data sets are getting rather large. However, for the milestone report the full
sets will be used.

Specific path and file names are set to load all 3 data files for further
analysis.

```{r load, include=FALSE}

# assign paths and file names
setwd("~/Coursera/Scripts and Data/Capstone")
data_dir <- file.path(getwd(), "datasets", "final", "en_US")

tw_file_name <- "en_US.twitter.txt"
bl_file_name <- "en_US.blogs.txt"
nw_file_name <- "en_US.news.txt"

# read the data 
twitter_data <- read_lines(paste0(data_dir, "/", tw_file_name),
                           skip_empty_rows = TRUE)
blogs_data <- read_lines(paste0(data_dir, "/", bl_file_name),
                   skip_empty_rows = TRUE)
news_data <- read_lines(paste0(data_dir, "/", nw_file_name),
                   skip_empty_rows = TRUE)

```

```{r table1}

# prepare a tibble
mytable1 <- tribble(
        ~data_source, ~number_lines,
        tw_file_name, length(twitter_data),
        bl_file_name, length(blogs_data),
        nw_file_name, length(news_data)
        )
mytable1

```

### Cleaning

The first step is to "tokenize" each data set (corpora) into smaller pieces.

We start at the word level which will allow us to analyze the count and
frequency of the words in each corpus.

Using the tokenization process provided with the tidytext package immediately
strips punctuation while converting all letters to lower case. In order to
reduce the number of words to "real" words we remove all words that do not start
with a letter.

```{r tokenization}

# prepare tibbles 
tw_text <- tibble(line = 1:length(twitter_data), text = twitter_data)
bl_text <- tibble(line = 1:length(blogs_data), text = blogs_data)
nw_text <- tibble(line = 1:length(news_data), text = news_data)

# tokenize these tibbles - separate into words 
tw_words <- tw_text %>%
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE)
bl_words <- bl_text %>% 
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE)
nw_words <- nw_text %>% 
        unnest_tokens(word, text) %>%
        filter(str_starts(word, "[a-z]") == TRUE)

```

For some kinds of analyses the removal of so-called stop words is recommended.
Stop words are the most common words like "the", "of", "to", etc. which may not
add much value in certain analyses. In our case, removing these stop words may
be counter-productive given the attempt to build a tool to predict the next
word.

## Exploratory Analysis

Table 2 shows the total number of lines and words extracted.

```{r table2}

# prepare tibble
mytable2 <- tribble(
        ~data_source, ~number_lines, ~total_number_words,
        "twitter", length(twitter_data), nrow(tw_words), 
        "blogs", length(blogs_data), nrow(bl_words), 
        "news", length(news_data), nrow(nw_words)
        )
mytable2

```

Given the much larger number of lines in the twitter data there are comparably
few words. This is likely caused by twitter's limited number of characters per
post.

In a next step we count the number of unique words and sort the data by
frequency.

```{r further_tokenization}

# add unique word count 
tw_words <- tw_words %>%
        count(word, sort = TRUE)
bl_words <- bl_words %>% 
        count(word, sort = TRUE)
nw_words <- nw_words %>% 
        count(word, sort = TRUE)

```

```{r table3}

# prepare tibble
mytable3 <- tribble(
        ~data_source, ~number_unique_words,
        "twitter", nrow(tw_words), 
        "blogs", nrow(bl_words), 
        "news", nrow(nw_words)
        )
mytable3

```

The next tables show the top 10 words.

Twitter

```{r tw_freq}

# add frequencies and cumulated frequencies
total_tw_words <- sum(tw_words$n)

tw_words <- tw_words %>%
        mutate(freq = n / total_tw_words) %>%
        mutate(cumfreq = cumsum(freq))

```

---
output: html_document
classoption:
- twocolumn
---

::: {.columns}
::: {.column width="50%"}
```{r tw_chart}

tw_words %>% 
  slice_head(n = 10)  %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(x = freq, y = word)) +
  geom_col() +
  labs(title = "Twitter: Top 10 words",
       y = NULL)

```
:::

::: {.column width="50%"}
```{r tw_table}

tw_words %>% slice_head(n = 10)

```
:::
:::

Blogs

```{r bl_freq}

total_bl_words <- sum(bl_words$n)

bl_words <- bl_words %>% 
        mutate(freq = n / total_bl_words) %>% 
        mutate(cumfreq = cumsum(freq))

```

---
output: html_document
classoption:
- twocolumn
---

::: {.columns}
::: {.column width="50%"}
```{r bl_chart}

bl_words %>% 
  slice_head(n = 10)  %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(x = freq, y = word)) +
  geom_col() +
  labs(title = "Blogs: Top 10 words",
       y = NULL)

```
:::

::: {.column width="50%"}
```{r bl_table4}

bl_words %>% slice_head(n = 10)

```
:::
:::

News

```{r nw_freq}

total_nw_words <- sum(nw_words$n)

nw_words <- nw_words %>% 
        mutate(freq = n / total_nw_words) %>% 
        mutate(cumfreq = cumsum(freq))

```

---
output: html_document
classoption:
- twocolumn
---

::: {.columns}
::: {.column width="50%"}
```{r nw_chart}

nw_words %>% 
  slice_head(n = 10) %>%
  mutate(word = reorder(word, freq)) %>%
  ggplot(aes(x = freq, y = word)) +
  geom_col() +
  labs(title = "News: Top 10 words",
       y = NULL)

```
:::

::: {.column width="50%"}
```{r nw_table4}

nw_words %>% slice_head(n = 10)

```
:::
:::

Top 10 bigrams (twitter)

```{r tw_2gram}

# create bigrams
tw_2grams <- tw_text %>%
        unnest_tokens(bigram, text, token = "ngrams", n = 2)  %>%
        count(bigram, sort = TRUE)

```

```{r tw_2gram_chart}

tw_2grams %>% 
  slice_head(n = 10)  %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = n, y = bigram)) +
  geom_col() +
  labs(title = "Twitter: Top 10 bigrams",
       y = NULL)

```

Top 10 trigrams (twitter)

```{r tw_3gram}

# create trigrams 
tw_3grams <- tw_text %>%
        unnest_tokens(trigram, text, token = "ngrams", n = 3)  %>%
        count(trigram, sort = TRUE)

```

```{r tw_3gram_chart}

tw_3grams %>% 
  slice_head(n = 10)  %>%
  mutate(trigram = reorder(trigram, n)) %>%
  ggplot(aes(x = n, y = trigram)) +
  geom_col() +
  labs(title = "Twitter: Top 10 trigrams",
       y = NULL)

```

## Interesting findings

### Finding 1

The corpora are reasonably different with respect to the words used as well as
their frequency. This raises the question whether the final application may
benefit from an assignment of a context specific text source. If the tool is to
be used in twitter or a chat application a next word prediction tool based on
the twitter data may give more appropriate results. In a similar line of
reasoning, a blogging application like wordpress could rely on the blogs data.

We tend to agree but may not follow this course in the capstone project.

### Finding 2

A naive implementation using the full dataset(s) may not be appropriate for time
and resource reasons. The run time for generating bigrams from twitter data is
about 3.5 minutes. Run time for building trigrams from the same source is about
10 minutes. Object sizes are 400 MB and 1.1 GB respectively. An application to
predict from a 3-word phrase would require four-grams. A similar increase in run
time and object size can be expected.


## Future Steps

find sensible reductions 
  sample x rows from twitter, blogs, and news into new corpus

Use the sbo package to create the application


Some thoughts on the Shiny app:
- Simple design
- input: text 
-- count number of words in input text if greater than 3 use only last 3
output: word (text)
logic:
if 3gram in list -> return most likely next word (or choice of 3?) 
  else use last 2 words 
  if 2gram in list -> return most likely next word 
    else use last word
    if word in list -> show most likely next word 
      else show most frequent word (?)

submit button
